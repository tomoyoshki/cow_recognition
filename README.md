# Cow Recognition

Author: Tomoyoshi Kimura

## Introduction

This is the cow recognition project that uses Faster RCNN techniques on tiles of an image. This project uses a pretrained FPN Resnet 50 model for object detection, and we uses tiles to increase the size of distant cows to increase the accuracy. There are mainly two parts of this project – Tiled Cow recognition (`tile_fasterrcnn.ipynb`) and coordinate conversion (`image_to_geolocation.ipynb`). 

**To install the dependencies**

```
pip3 install -r requirements.txt
```

### Directory

To ensure that the program works as expected, the following format of the directory is strongly recommended

```
cow_recognition
├── cow_images
	├── season_1
		├── image_1
		├── image_2
		├── ...
		├── image_n
	├── season_2
		├── ...
	├── season_3
		├── ...
	....
	├── season_n
		├── ...
├── image_to_geolocation.ipynb
├── requirements.txt
└── tile_fasterrcnn.ipynb
```

## Tile Faster RCNN Cow Detection

For Cow Detection, since there are many cows that are distant away, I decided to implement a tiling method that slice an images into many tiles, and use the model on that specific tile and then glue everything back together. 

### Cow Detection Directory 

#### Input

```
cow_recognition
├── cow_images
	├── season_1
		├── image_1
		├── image_2
		├── ...
		├── image_n
	├── season_2
		├── ...
	├── season_3
		├── ...
	....
	├── season_n
		├── ...
└── tile_fasterrcnn.ipynb
```

#### Output

```
cow_recognition
├── cow_boxed_images
	├── boxed_season_1
		├── boxed_image_1
		├── boxed_image_2
		├── ...
		├── boxed_image_n
	├── boxed_season_2
		├── ...
	├── boxed_season_3
		├── ...
	....
	├── boxed_season_n
		├── ...
├── cow_count_pixel_location.txt
└── tile_fasterrcnn.ipynb
```

### Detection Pipeline

Simply run the entire notebook, and the pipeline looks like the following

1. Load **Resnet50 Faster RCNN Model**
   1. Define the COCO Dataset and desired object classes
2. Loop through each files
   1. **Slice** the original image into smaller images (sliced images)
   2. Loop through each sliced image
      1. Use the **sliced image as the input** to the model
      2. Model outputs predicted classes and bounding boxes if there is any
      3. **Convert** the **bounding box coordinates of the sliced image into the original image coordinate**
      4. Draw rectangles and labels on the original image
      5. **Delete** the sliced image
   3. When all sliced images are done, return the labeled images and the number of bounding boxes
   4. Save the image and write the result to the files

### Improvements

1. Some bounding boxes overlap, meaning that a cow is counted twice and have two bounding boxes overlapping with each other entirely, could try to delete bounding boxes with > 95% overlapping
2. Some cows are sliced by half, half of the cow is in the left sliced image and half of it is in the right sliced, may result in two cows counted in two images. 

## Image coordinate to Geolocation

This program converts the image coordinate of each cow into real world coorindate with the given camera coordinate and the direction

### Image coordinate to Geolocation directory

#### Input

```
cow_recognition
├── cow_count_pixel_location.txt (from part 1)
└── image_to_geolocation.ipynb
```

#### Output

```
cow_recognition
├── cow_geolocation_result.csv
└── cow_geolocation_result.txt
```

### Conversion Pipeline

1. Read in the text file input generated by the first part (detection)
2. Loop through each file
   1. Get the camera type and direction
   2. Use the camera type and direction to obtain rotational matrix for the position
   3. For each bounding box
      1. Obtain the relative position with row and column (I used top left corner) using the formula provided by Chongya
      2. Apply the rotational matrices and the relative position to obtain the change in the vertical direction and horizontal location in meters.
      3. Convert meters into latitude and longitude
      4. Add to the original position

### Improvements

- Calibration metrics for each camera
  - Assumed same heights and completely the same camera
- Assumed the angles for NW NE SW SE are 45 degrees with respect to the horizontal axis